#import "@preview/charged-ieee:0.1.3": ieee
#import "@local/david:1.0.0": *

#show: ieee.with(
  title: [CS221 Project Progress Report:\ Understanding Recent Bandit Algorithms],
  authors: (
    (
      name: "David Chen",
      organization: [Stanford University],
      email: "dchen11@stanford.edu"
    ),
  ),
  bibliography: bibliography("refs.bib"),
  figure-supplement: [Fig.],
)

= Introduction

In the broad context of online decision-making under uncertainty, the class of problems known as multi-armed bandits (MABs) has provided a rich environment for insightful theoretical analysis and applicable algorithms. Multi-armed bandit problems have been studied in fields ranging from computer science and electrical engineering, to operations research and economics. Although closely related to the more general setting of reinforcement learning, study of MABs typically has a strong focus on the classic dilemma of exploration-exploitation. Fundamental insights from theoretical developments have been further developed for more complex reinforcement learning settings, and simple yet effective bandit algorithms have been deployed for practical use-cases in the industry with great success.

== Outline
- Goal of project
- Overview of general bandit problem
- Overview of some basic fundamental ideas (regret bounds) and bandit settings
- My specific focus.

= Related Works
- lai and robbins 1985
- info theoretic analysis thompson sampling
- learning to optimize via IDS
- thompson 1933
- bayes ucb
- Finite-time analysis of the multiarmed bandit
problem (UCB1)
- An empirical evaluation of Thompson sampling
- bandit formulation Some aspects of the sequential design of experiments
- bandit algorithms
- slivkin bandit stuff

= Methodology

== Problem formulation

== Dataset

== Baseline
- I be implemented bernouliiiii
- And thompson sampling
- and urrmmmmm 

== Main approach

== Evaluation

= Discussion

= Future Work

